\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

%\jmlrheading{1}{2017}{start-end}{9/17}{9/17}{meila00a}{Baker, Bonney, White}

% Short headings should be running head and authors last names

\ShortHeadings{Converting data to ARFF}{Baker, Bonney, White}
\firstpageno{1}

\begin{document}

\title{Learning with Mixtures of Trees}

\author{\name{Trent Baker} \email{b.trent5@gmail.com} \\
       \AND
       \name{Logan Bonney} \email{wakeup2early@gmail.com} \\
       \AND
       \name{Bradley White} \email{tttt@tttt.com} \\
       }

\editor{John W. Sheppard}

\maketitle

\begin{abstract}%

\end{abstract}

\begin{keywords}
  adsfasdf, dfasd, gfs, dgfgsad, asdfg
\end{keywords}

%include hypothesis in here
\section{Introduction}
There are many different classification algorithms that can be used to learn from a training set. Some algorithms support different data types and work better on certain classification problems. Our goal was to train and tweak 5 algorithms to be as accurate as possible, then compare how they preformed so we can getting a better understanding of how algorithms compare.

\section{Descrptions of Algorithms Used}

\section{Experiment Design}

\section{Results}

\section{Conclusion}

\section{Summary}
% Acknowledgements should go at the end, before appendices and references
%\acks{fd}

\appendix
\section*{Appendix A.}

% \bibliography{sample}

\end{document}
